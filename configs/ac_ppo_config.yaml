experiment:
  # env examples:
    # "MiniGrid-FourRooms-v0"
    # "CustomMountainCar-v0"
    # "CartPole-v1"
  env_import: "CartPole-v1"
  name: "norm_4"
  description: "PPO Experiment"
  save_path: "./runs"
  num_envs: 4

environment:
  # init_low: 0.0
  # init_high: 0.0
  reward_type: "min"  # min / mean / sum
  termination_type: "first"  # first / half / all

normalization:
  enabled: true
  norm_obs: true
  norm_reward: true

algorithm:
  name: "PPO"
  
  # Core hyperparameters
  hyperparameters:
    learning_rate: 3.0e-4
    batch_size: 256
    n_epochs: 10
    n_steps: 2048
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    ent_coef: 0.0
    vf_coef: 0.5
    max_grad_norm: 0.5

  # Network architecture
  policy_kwargs:
    net_arch: [64, 64]
    activation_fn: "tanh"
  
  # Advanced settings
  use_sde: false
  sde_sample_freq: -1

training:
  timesteps: 20000   # used for train and resume
  
  # Evaluation during training
  eval_freq: 5000
  n_eval_episodes: 5
  eval_deterministic: true
  
  # Checkpointing
  save_freq: 5000
  save_best_model: true
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 5
    min_delta: 10.0

logging:
  # Logging backends
  tensorboard: true
  wandb: false
  console_log_level: "INFO"
  
  # Video recording
  record_video: true
  video_freq: 50000
  video_length: 500
  
  # Custom metrics
  custom_metrics:
    - "episode_height"
    - "exploration_bonus"

# Testing/evaluation settings
evaluation:
  episodes: 3
  model_type: "final"
  deterministic: true
  render_mode: "rgb_array"
  save_trajectories: false


# Reproducibility
seed: 42
deterministic_pytorch: true