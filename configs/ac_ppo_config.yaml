experiment:
  # env examples:
    # "MiniGrid-FourRooms-v0"
    # "CustomMountainCar-v0"
    # "CartPole-v1"
    # "Pendulum-v1"
  env_import: "CellGroup-v0"
  name: "staying_alive_1"
  description: "PPO Experiment"
  save_path: "./runs"
  num_envs: 1

environment:
  init_low:
    tplus_counts: 100      # T+ cells minimum count
    tprod_counts: 100     # TP cells minimum count  
    tneg_counts: 1.0e-9   # T- cells minimum count (near zero)
    
  init_high:
    tplus_counts: 4000    # T+ cells maximum count
    tprod_counts: 4000   # TP cells maximum count
    tneg_counts: 1.0e-6      # T- cells maximum count

  reward_type: "min"  # min / avg / sum
  termination_type: "first"  # first / half / all

# normalization:
  enabled: false
  norm_obs: false
  norm_reward: false

algorithm:
  name: "PPO"
  
  # Core hyperparameters
  hyperparameters:
    learning_rate: 0.004
    batch_size: 128
    n_epochs: 4
    n_steps: 4096
    gamma: 0.998
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    ent_coef: 0.025
    vf_coef: 0.5
    max_grad_norm: 0.5
    use_sde: false
    sde_sample_freq: -1
    target_kl: null

  tuning:
    enabled: true
    num_trials: 10
    search_space:
      learning_rate: [0.001, 0.01]
      gamma: [0.993, 0.9995]
      gae_lambda: [0.8, 0.98]
      clip_range: [0.1, 0.3]
      ent_coef: [0.0, 0.1]

training:
  timesteps: 300000   # used for train and resume
  
  # Evaluation during training
  eval_freq: 50000
  n_eval_episodes: 5
  eval_deterministic: true
  
  # Checkpointing
  save_freq: 100000
  save_best_model: true

# Testing/evaluation settings
evaluation:
  episodes: 3
  model_type: "best"
  deterministic: true
  render_mode: "rgb_array"
  save_trajectories: false

# Reproducibility
seed: 42
deterministic_pytorch: true