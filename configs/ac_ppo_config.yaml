experiment:
  # env examples:
    # "MiniGrid-FourRooms-v0"
    # "CustomMountainCar-v0"
    # "CartPole-v1"
    # "Pendulum-v1"
  env_import: "CellGroup-v0"
  name: "new_norm_3M_try5"
  description: "PPO Experiment"
  save_path: "./runs"
  num_envs: 9

environment:
  agent_type: "spatial"  # spatial / bulk
  init_state:
    tplus_counts: [25, 25, 50, 300, 700, 200, 800, 24, 300]      # T+ cells minimum count
    tprod_counts: [800, 200, 100, 30, 400, 500, 200, 600, 200]     # TP cells minimum count  
    tneg_counts: [3.0e-6, 1.0e-6, 4.0e-6, 1.0e-6, 1.0e-6, 1.0e-6, 9.0e-7, 1.0e-6, 3.0e-6]   # T- cells minimum count (near zero)

  # init_low:
  #   tplus_counts: 2424      # T+ cells minimum count
  #   tprod_counts: 3030     # TP cells minimum count  
  #   tneg_counts: 1.0e-9   # T- cells minimum count (near zero)
    
  # init_high:
  #   tplus_counts: 2424    # T+ cells maximum count
  #   tprod_counts: 3030   # TP cells maximum count
  #   tneg_counts: 1.0e-9      # T- cells maximum count

  stochastic_action: false
  reward_type: "avg"  # min / avg / sum
  termination_type: null  # first / half / all
  # terminating when total population reaches x1.2 from starting population

normalization:
  enabled: true
  norm_obs: true
  norm_reward: false

algorithm:
  name: "PPO"
  
  # Core hyperparameters
  hyperparameters:
    learning_rate: 3.0e-4
    batch_size: 256
    n_epochs: 10
    n_steps: 8192
    gamma: 0.9999
    gae_lambda: 0.98
    clip_range: 0.2
    clip_range_vf: null
    ent_coef: 0.02
    vf_coef: 0.5
    max_grad_norm: 0.5
    use_sde: false
    sde_sample_freq: -1
    target_kl: null

    features_dim: 64
    d_model: 32
    num_heads: 2
    num_layers: 1
    dropout: 0.1

  tuning:
    enabled: true
    num_trials: 5
    search_space:
      learning_rate: ['log', 8.0e-6, 5.0e-4]
      # gamma: ['log', 0.99, 0.999]
      # gae_lambda: ['linear', 0.90, 0.99]
      # clip_range: ['linear', 0.05, 0.25]
      # ent_coef: ['linear', 0.03, 0.055]

training:
  timesteps: 3000000   # used for train and resume
  
  # Evaluation during training
  eval_freq: 25000
  n_eval_episodes: 1  # one is enough when initial state is fixed
  eval_deterministic: true
  
  # Checkpointing
  save_freq: 600000
  save_best_model: true

# Testing/evaluation settings
evaluation:
  eval_type: "visual"  # visual / numeric
  test_only_fixed_policy: false
  fixed_policy_type: "MTD"  # MTD / Adaptive / Optimal
  episodes: 1
  model_type: "best"
  model_path: null
  trial_to_eval: null
  deterministic: true
  render_mode: "rgb_array"
  save_trajectories: false

# Reproducibility
seed: 42
deterministic_pytorch: true