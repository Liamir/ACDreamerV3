experiment:
  env_import: "CustomMountainCar-v0"
  name: "hp1"
  description: "DQN Experiment"
  save_path: "./runs"
  num_envs: 4
  
environment:
  init_low: -1.2
  init_high: 0.4
  # Custom initialization ranges (optional)
  reward_type: "min"  # min / mean / sum
  termination_type: "all"  # first / half / all

  # Environment-specific settings
  custom_params:
    goal_velocity: 0.0
    height_reward_scale: 1.0
  
  # Wrappers (applied in order)
  wrappers:
    - name: "VecNormalize"
      normalize_obs: true
      normalize_reward: true
    - name: "VecFrameStack"
      n_stack: 1

algorithm:
  name: "DQN"
  
  # Core hyperparameters
  hyperparameters:
    learning_rate: 0.001
    buffer_size: 50000
    learning_starts: 1000
    batch_size: 128
    target_update_interval: 600
    train_freq: 4
    gradient_steps: 1
    exploration_fraction: 0.3
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.02
    gamma: 0.99
  
  # Network architecture
  policy_kwargs:
    net_arch: [64, 64]
    activation_fn: "tanh"
  
  # Advanced settings
  use_sde: false
  sde_sample_freq: -1

training:
  timesteps: 200000   # used for train and resume
  
  # Evaluation during training
  eval_freq: 15000
  n_eval_episodes: 5
  eval_deterministic: true
  
  # Checkpointing
  save_freq: 15000
  save_best_model: true
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 5
    min_delta: 10.0

logging:
  # Logging backends
  tensorboard: true
  wandb: false
  console_log_level: "INFO"
  
  # Video recording
  record_video: true
  video_freq: 50000
  video_length: 500
  
  # Custom metrics
  custom_metrics:
    - "episode_height"
    - "exploration_bonus"

# Testing/evaluation settings
evaluation:
  episodes: 3
  model_type: "best"
  deterministic: true
  render_mode: "rgb_array"
  save_trajectories: false


# Reproducibility
seed: 42
deterministic_pytorch: true