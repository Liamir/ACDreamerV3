experiment:
  env_import: "CustomMountainCar-v0"
  name: "height_reward"
  description: "PPO on MountainCar with height-based rewards"
  save_path: "./runs"
  num_envs: 1
  
  # Environment-specific settings
  custom_params:
    goal_velocity: 0.0
    height_reward_scale: 1.0
  
  # Wrappers (applied in order)
  wrappers:
    - name: "VecNormalize"
      normalize_obs: true
      normalize_reward: true
    - name: "VecFrameStack"
      n_stack: 1

algorithm:
  name: "PPO"
  
  # Core hyperparameters
  hyperparameters:
    learning_rate: 3.0e-4
    batch_size: 64
    n_epochs: 10
    n_steps: 2048
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    ent_coef: 0.0
    vf_coef: 0.5
    max_grad_norm: 0.5
  
  # Network architecture
  policy_kwargs:
    net_arch: [64, 64]
    activation_fn: "tanh"
  
  # Advanced settings
  use_sde: false
  sde_sample_freq: -1

training:
  total_timesteps: 10000
  
  # Evaluation during training
  eval_freq: 1000
  n_eval_episodes: 10
  eval_deterministic: true
  
  # Checkpointing
  save_freq: 5000
  save_best_model: true
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 5
    min_delta: 10.0

logging:
  # Logging backends
  tensorboard: true
  wandb: false
  console_log_level: "INFO"
  
  # Video recording
  record_video: true
  video_freq: 50000
  video_length: 500
  
  # Custom metrics
  custom_metrics:
    - "episode_height"
    - "exploration_bonus"

# Testing/evaluation settings
evaluation:
  episodes: 2
  model_type: "best"
  deterministic: true
  render_mode: "rgb_array"
  save_trajectories: false


# Reproducibility
seed: 42
deterministic_pytorch: true